{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0bd757ea-a6a0-43f8-8701-cafb44f20f6b",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook serves as a guide for new developers using the `chebai` package. If you just want to run the experiments, you can refer to the [README.md](https://github.com/ChEB-AI/python-chebai/blob/dev/README.md) and the [wiki](https://github.com/ChEB-AI/python-chebai/wiki) for the basic commands. This notebook explains what happens under the hood for the SCOPe dataset. It covers\n",
    "- how to instantiate a data class and generate data\n",
    "- how the data is processed and stored\n",
    "- and how to work with different molecule encodings.\n",
    "\n",
    "The `chebai` package simplifies the handling of these datasets by **automatically downloading and processing** them as needed. This means that you do not have to input any data manually; the package will generate and organize the data files based on the parameters and encodings selected. You can however provide your own data files, for instance if you want to replicate a specific experiment.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca637ce-d4ea-4365-acd9-657418e0640f",
   "metadata": {},
   "source": [
    "### Overview of SCOPe Data and its Usage in Protein-Related Tasks\n",
    "\n",
    "#### **What is SCOPe?**\n",
    "\n",
    "The **Structural Classification of Proteins â€” extended (SCOPe)** is a comprehensive database that extends the original SCOP (Structural Classification of Proteins) database. SCOPe offers a detailed classification of protein domains based on their structural and evolutionary relationships.\n",
    "\n",
    "The SCOPe database, like SCOP, organizes proteins into a hierarchy of domains based on structural similarities, which is crucial for understanding evolutionary patterns and functional aspects of proteins. This hierarchical structure is comparable to taxonomy in biology, where species are classified based on shared characteristics.\n",
    "\n",
    "#### **SCOPe Hierarchy:**\n",
    "By analogy with taxonomy, SCOP was created as a hierarchy of several levels where the <u>fundamental unit of classification is a **domain** </u> in the experimentally determined protein structure. Starting at the bottom, the hierarchy of SCOP domains comprises the following levels:\n",
    "\n",
    "1. **Species**: Representing distinct protein sequences and their naturally occurring or artificially created variants.\n",
    "2. **Protein**: Groups together similar sequences with essentially the same functions. These can originate from different biological species or represent isoforms within the same species.\n",
    "3. **Family**: Contains proteins with similar sequences but typically distinct functions.\n",
    "4. **Superfamily**: Bridges protein families with common functional and structural features, often inferred from a shared evolutionary ancestor.\n",
    "5. **Fold**: Groups structurally similar superfamilies. \n",
    "6. **Class**: Based on secondary structure content and organization. This level classifies proteins based on their secondary structure properties, such as alpha-helices and beta-sheets.\n",
    "\n",
    "\n",
    "\n",
    "For more details, you can refer to the [SCOPe documentation](https://scop.berkeley.edu/help/ver=2.08).\n",
    "\n",
    "---\n",
    "\n",
    "#### **Why are We Using SCOPe?**\n",
    "\n",
    "We are integrating the SCOPe data into our pipeline as part of an ontology pretraining task for protein-related models. SCOPe is a great fit for our goal because it is primarily **structure-based**, unlike other protein-related databases like Gene Ontology (GO), which focuses more on functional classes.\n",
    "\n",
    "Our primary objective is to reproduce **ontology pretraining** on a protein-related task, and SCOPe provides the structural ontology that we need for this. The steps in our pipeline are aligned as follows:\n",
    "\n",
    "| **Stage**                | **Chemistry Task**                  | **Proteins Task**                              |\n",
    "|--------------------------|-------------------------------------|------------------------------------------------|\n",
    "| **Unsupervised Pretraining** | Mask pretraining (ELECTRA)         | Mask pretraining (ESM2, optional)              |\n",
    "| **Ontology Pretraining** | ChEBI                               | SCOPe                                          |\n",
    "| **Finetuning Task**     | Toxicity, Solubility, etc.          | GO (MF, BP, CC branches)                      |\n",
    "\n",
    "                                                                                                                                                        \n",
    "This integration will allow us to use **SCOPe** for tasks such as **protein classification** and will contribute to the success of **pretraining models** for protein structures. The data will be processed with the same approach as the GO data, with **different labels** corresponding to the SCOPe classification system.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Why SCOPe is Suitable for Our Task**\n",
    "\n",
    "1. **Structure-Based Classification**: SCOPe is primarily concerned with the structural characteristics of proteins, making it ideal for protein structure pretraining tasks. This contrasts with other ontology databases like **GO**, which categorize proteins based on more complex functional relationships.\n",
    "   \n",
    "2. **Manageable Size**: SCOPe contains around **140,000 entries**, making it a manageable dataset for training models. This is similar in size to **ChEBI**, which is used in the chemical domain, and ensures we can work with it effectively for pretraining."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338e452f-426c-493d-bec2-5bd51e24e4aa",
   "metadata": {},
   "source": [
    "\n",
    "### Protein Data Bank (PDB)\n",
    "\n",
    "The **Protein Data Bank (PDB)** is a global repository that stores 3D structural data of biological macromolecules like proteins and nucleic acids. It contains information obtained through experimental methods such as **X-ray crystallography**, **NMR spectroscopy**, and **cryo-EM**. The data includes atomic coordinates, secondary structure details, and experimental conditions.\n",
    "\n",
    "The PDB is an essential resource for **structural biology**, **bioinformatics**, and **drug discovery**, enabling scientists to understand protein functions, interactions, and mechanisms at the molecular level.\n",
    "\n",
    "For more details, visit the [RCSB PDB website](https://www.rcsb.org/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c25706-251c-438c-9915-e8002647eb94",
   "metadata": {},
   "source": [
    "### Understanding [SCOPe](https://scop.berkeley.edu/) and [PDB](https://www.rcsb.org/)  \n",
    "\n",
    "\n",
    "1. **Protein domains form chains.**  \n",
    "2. **Chains form complexes** (protein complexes or structures).  \n",
    "3. These **complexes are the entries in PDB**, represented by unique identifiers like `\"1A3N\"`.  \n",
    "\n",
    "---\n",
    "\n",
    "#### **Protein Domain**  \n",
    "A **protein domain** is a **structural and functional unit** of a protein.  \n",
    "\n",
    "\n",
    "##### Key Characteristics:\n",
    "- **Domains are part of a protein chain.**  \n",
    "- A domain can span:  \n",
    "  1. **The entire chain** (single-domain protein):  \n",
    "     - In this case, the protein domain is equivalent to the chain itself.  \n",
    "     - Example:  \n",
    "       - All chains of the **PDB structure \"1A3N\"** are single-domain proteins.  \n",
    "       - Each chain has a SCOPe domain identifier. \n",
    "       - For example, Chain **A**:  \n",
    "         - Domain identifier: `d1a3na_`  \n",
    "         - Breakdown of the identifier:  \n",
    "           - `d`: Denotes domain.  \n",
    "           - `1a3n`: Refers to the PDB protein structure identifier.  \n",
    "           - `a`: Specifies the chain within the structure.  (`_` for None and `.` for multiple chains)\n",
    "           - `_`: Indicates the domain spans the entire chain (single-domain protein).  \n",
    "         - Example: [PDB Structure 1A3N - Chain A](https://www.rcsb.org/sequence/1A3N#A)\n",
    "  2. **A specific portion of the chain** (multi-domain protein):  \n",
    "     - Here, a single chain contains multiple domains.  \n",
    "     - Example: Chain **A** of the **PDB structure \"1PKN\"** contains three domains: `d1pkna1`, `d1pkna2`, `d1pkna3`.  \n",
    "     - Example: [PDB Structure 1PKN - Chain A](https://www.rcsb.org/annotations/1PKN).  \n",
    "\n",
    "---\n",
    "\n",
    "#### **Protein Chain**  \n",
    "A **protein chain** refers to the entire **polypeptide chain** observed in a protein's 3D structure (as described in PDB files).  \n",
    "\n",
    "##### Key Points:\n",
    "- A chain can consist of **one or multiple domains**:\n",
    "  - **Single-domain chain**: The chain and domain are identical.  \n",
    "    - Example: Myoglobin.  \n",
    "  - **Multi-domain chain**: Contains several domains, each with distinct structural and functional roles.  \n",
    "- Chains assemble to form **protein complexes** or **structures**.  \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "#### **Key Observations About SCOPe**  \n",
    "- The **fundamental classification unit** in SCOPe is the **protein domain**, not the entire protein.  \n",
    "- _**The taxonomy in SCOPe is not for the entire protein (i.e., the full-length amino acid sequence as encoded by a gene) but for protein domains, which are smaller, structurally and functionally distinct regions of the protein.**_\n",
    "\n",
    "\n",
    "--- \n",
    "\n",
    "**SCOPe 2.08 Data Analysis:**\n",
    "\n",
    "The current SCOPe version (2.08) includes the following statistics based on analysis for relevant data:\n",
    "\n",
    "- **Classes**: 12\n",
    "- **Folds**: 1485\n",
    "- **Superfamilies**: 2368\n",
    "- **Families**: 5431\n",
    "- **Proteins**: 13,514\n",
    "- **Species**: 30,294\n",
    "- **Domains**: 344,851\n",
    "\n",
    "For more detailed statistics, please refer to the official SCOPe website:\n",
    "\n",
    "- [SCOPe 2.08 Statistics](https://scop.berkeley.edu/statistics/ver=2.08)\n",
    "- [SCOPe 2.08 Release](https://scop.berkeley.edu/ver=2.08)\n",
    "\n",
    "---\n",
    "\n",
    "## SCOPe Labeling \n",
    "\n",
    "- Use SCOPe labels for protein domains.\n",
    "- Map them back to their **protein-chain** sequences (protein sequence label = sum of all domain labels).\n",
    "- Train on protein sequences.\n",
    "- This pretraining task would be comparable to GO-based training.\n",
    "\n",
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "990cc6f2-6b4a-4fa7-905f-dda183c3ec4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changed to project root directory: G:\\github-aditya0by0\\python-chebai\n"
     ]
    }
   ],
   "source": [
    "# To run this notebook, you need to change the working directory of the jupyter notebook to root dir of the project.\n",
    "import os\n",
    "\n",
    "# Root directory name of the project\n",
    "expected_root_dir = \"python-chebai\"\n",
    "\n",
    "# Check if the current directory ends with the expected root directory name\n",
    "if not os.getcwd().endswith(expected_root_dir):\n",
    "    os.chdir(\"..\")  # Move up one directory level\n",
    "    if os.getcwd().endswith(expected_root_dir):\n",
    "        print(\"Changed to project root directory:\", os.getcwd())\n",
    "    else:\n",
    "        print(\"Warning: Directory change unsuccessful. Current directory:\", os.getcwd())\n",
    "else:\n",
    "    print(\"Already in the project root directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4550d01fc7af5ae4",
   "metadata": {},
   "source": [
    "# 1. Instantiation of a Data Class\n",
    "\n",
    "To start working with `chebai`, you first need to instantiate a SCOPe data class. This class is responsible for managing, interacting with, and preprocessing the ChEBI chemical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3a66e07-edc9-4aa2-9cd0-d4ea58914d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from chebai.preprocessing.datasets.scope.scope import SCOPeOver50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a71b7301-6195-4155-a439-f5eb3183d0f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-05T21:07:26.371796Z",
     "start_time": "2024-10-05T21:07:26.058728Z"
    }
   },
   "outputs": [],
   "source": [
    "scope_class = SCOPeOver50(scope_version=\"2.08\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b810d7c9-4f7f-4725-9bc2-452ff2c3a89d",
   "metadata": {},
   "source": [
    "\n",
    "### Inheritance Hierarchy\n",
    "\n",
    "SCOPe data classes inherit from [`_DynamicDataset`](https://github.com/ChEB-AI/python-chebai/blob/dev/chebai/preprocessing/datasets/base.py#L598), which in turn inherits from [`XYBaseDataModule`](https://github.com/ChEB-AI/python-chebai/blob/dev/chebai/preprocessing/datasets/base.py#L23). Specifically:\n",
    "\n",
    "- **`_DynamicDataset`**: This class serves as an intermediate base class that provides additional functionality or customization for datasets that require dynamic behavior. It inherits from `XYBaseDataModule`, which provides the core methods for data loading and processing.\n",
    "\n",
    "- **`XYBaseDataModule`**: This is the base class for data modules, providing foundational properties and methods for handling and processing datasets, including data splitting, loading, and preprocessing.\n",
    "\n",
    "In summary, ChEBI data classes are designed to manage and preprocess chemical data effectively by leveraging the capabilities provided by `XYBaseDataModule` through the `_DynamicDataset` intermediary.\n",
    "\n",
    "\n",
    "### Input parameters\n",
    "A SCOPe data class can be configured with a range of parameters, including:\n",
    "\n",
    "- **scope_version (str)**: Specifies the version of the ChEBI database to be used. Specifying a version ensures the reproducibility of your experiments by using a consistent dataset.\n",
    "\n",
    "- **scope_version_train (str, optional)**: The version of ChEBI to use specifically for training and validation. If not set, the `scope_version` specified will be used for all data splits, including training, validation, and test. Defaults to `None`.\n",
    "\n",
    "- **splits_file_path (str, optional)**: Path to a CSV file containing data splits. If not provided, the class will handle splits internally. Defaults to `None`.\n",
    "\n",
    "### Additional Input Parameters\n",
    "\n",
    "To get more control over various aspects of data loading, processing, and splitting, you can refer to documentation of additional parameters in docstrings of the respective classes: [`_SCOPeDataExtractor`](https://github.com/ChEB-AI/python-chebai/blob/dev/chebai/preprocessing/datasets/scope/scope.py#L31), [`XYBaseDataModule`](https://github.com/ChEB-AI/python-chebai/blob/dev/chebai/preprocessing/datasets/base.py#L22), [`_DynamicDataset`](https://github.com/ChEB-AI/python-chebai/blob/dev/chebai/preprocessing/datasets/base.py#L597), etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8578b7aa-1bd9-4e50-9eee-01bfc6d5464a",
   "metadata": {},
   "source": [
    "# Available SCOPe Data Classes\n",
    "\n",
    "__Note__: Check the code implementation of classes [here](https://github.com/ChEB-AI/python-chebai/blob/dev/chebai/preprocessing/datasets/scope/scope.py):\n",
    "\n",
    "There is a range of available dataset classes for SCOPe. Usually, you want to use `SCOPeOver2000` or `SCOPeOver50`. The number indicates the threshold for selecting label classes: SCOPe classes which have at least 2000 / 50 subclasses will be used as labels.\n",
    "\n",
    "Both inherit from `SCOPeOverX`. If you need a different threshold, you can create your own subclass. By default, `SCOPeOverX` uses the Protein encoding (see Section 5).\n",
    "\n",
    "Finally, `SCOPeOver2000Partial` selects extracts a part of SCOPe based on a given top class, with a threshold of 2000 for selecting labels.\n",
    "This class inherits from `SCOPEOverXPartial`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8456b545-88c5-401d-baa5-47e8ae710f04",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed973fb59df11849",
   "metadata": {},
   "source": [
    "# 2. Preparation / Setup Methods\n",
    "\n",
    "Now we have a SCOPe data class with all the relevant parameters. Next, we need to generate the actual dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11f2208e-fa40-44c9-bfe7-576ca23ad366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for processed data in data\\SCOPe\\version_2.08\\SCOPe50\\processed\n",
      "Missing processed data file (`data.pkl` file)\n",
      "Missing PDB raw data, Downloading PDB sequence data....\n",
      "Downloading to temporary file C:\\Users\\HP\\AppData\\Local\\Temp\\tmpsif7r129\n",
      "Downloaded to C:\\Users\\HP\\AppData\\Local\\Temp\\tmpsif7r129\n",
      "Unzipping the file....\n",
      "Unpacked and saved to data\\SCOPe\\pdb_sequences.txt\n",
      "Removed temporary file C:\\Users\\HP\\AppData\\Local\\Temp\\tmpsif7r129\n",
      "Missing Scope: cla.txt raw data, Downloading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\anaconda3\\envs\\env_chebai\\lib\\site-packages\\urllib3\\connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'scop.berkeley.edu'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Scope: hie.txt raw data, Downloading...\n",
      "Missing Scope: des.txt raw data, Downloading...\n",
      "Extracting class hierarchy...\n",
      "Computing transitive closure\n",
      "Process graph\n",
      "101 labels has been selected for specified threshold, \n",
      "Constructing data.pkl file .....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Check for processed data in data\\SCOPe\\version_2.08\\SCOPe50\\processed\\protein_token\n",
      "Cross-validation enabled: False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing transformed data (`data.pt` file). Transforming data.... \n",
      "Processing 60298 lines...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 60298/60298 [00:53<00:00, 1119.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving 21 tokens to G:\\github-aditya0by0\\python-chebai\\chebai\\preprocessing\\bin\\protein_token\\tokens.txt...\n",
      "First 10 tokens: ['M', 'S', 'I', 'G', 'A', 'T', 'R', 'L', 'Q', 'N']\n"
     ]
    }
   ],
   "source": [
    "scope_class.prepare_data()\n",
    "scope_class.setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1655d489-25fe-46de-9feb-eeca5d36936f",
   "metadata": {},
   "source": [
    "\n",
    "### Automatic Execution: \n",
    "These methods are executed automatically when using the training command `chebai fit`. Users do not need to call them explicitly, as the code internally manages the preparation and setup of data, ensuring that it is ready for subsequent use in training and validation processes.\n",
    "\n",
    "### Why is Preparation Needed?\n",
    "\n",
    "- **Data Availability**: The preparation step ensures that the required SCOPe data files are downloaded or loaded, which are essential for analysis.\n",
    "- **Data Integrity**: It ensures that the data files are transformed into a compatible format required for model input.\n",
    "\n",
    "### Main Methods for Data Preprocessing\n",
    "\n",
    "The data preprocessing in a data class involves two main methods:\n",
    "\n",
    "1. **`prepare_data` Method**:\n",
    "   - **Purpose**: This method checks for the presence of raw data in the specified directory. If the raw data is missing, it fetches the ontology, creates a dataframe, and saves it to a file (`data.pkl`). The dataframe includes columns such as IDs, data representations, and labels. This step is independent of input encodings.\n",
    "   - **Documentation**: [PyTorch Lightning - `prepare_data`](https://lightning.ai/docs/pytorch/stable/data/datamodule.html#prepare-data)\n",
    "\n",
    "2. **`setup` Method**:\n",
    "   - **Purpose**: This method sets up the data module for training, validation, and testing. It checks for the processed data and, if necessary, performs additional setup to ensure the data is ready for model input. It also handles cross-validation settings if enabled.\n",
    "   - **Description**: Transforms `data.pkl` into a model input data format (`data.pt`), tokenizing the input according to the specified encoding. The transformed data contains the following keys: `ident`, `features`, `labels`, and `group`. This method uses a subclass of Data Reader to perform the tokenization.\n",
    "   - **Documentation**: [PyTorch Lightning - `setup`](https://lightning.ai/docs/pytorch/stable/data/datamodule.html#setup)\n",
    "\n",
    "These methods ensure that the data is correctly prepared and set up for subsequent use in training and validation processes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5aaa12d-5f01-4b74-8b59-72562af953bf",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6e9a81554368f7",
   "metadata": {},
   "source": [
    "# 3. Overview of the 3 preprocessing stages\n",
    "\n",
    "The `chebai` library follows a three-stage preprocessing pipeline, which is reflected in its file structure:\n",
    "\n",
    "1. **Raw Data Stage**:\n",
    "   - **Files**: `cla.txt`, `des.txt` and `hie.txt`. Please find description of each file [here](https://scop.berkeley.edu/help/ver=2.08#parseablefiles-2.08).\n",
    "   - **Description**: This stage contains the raw SCOPe data in txt format, serving as the initial input for further processing.\n",
    "   - **File Path**: `data/SCOPe/version_${scope_version}/raw/${filename}.txt`\n",
    "\n",
    "2. **Processed Data Stage 1**:\n",
    "   - **File**: `data.pkl`\n",
    "   - **Description**: This stage includes the data after initial processing. It contains protein sequence strings, class columns, and metadata but lacks data splits.\n",
    "   - **File Path**: `data/SCOPe/version_${scope_version}/${dataset_name}/processed/data.pkl`\n",
    "   - **Additional File**: `classes.txt` - A file listing the relevant SCOPe classes.\n",
    "\n",
    "3. **Processed Data Stage 2**:\n",
    "   - **File**: `data.pt`\n",
    "   - **Description**: This final stage includes the encoded data in a format compatible with PyTorch, ready for model input. This stage also references data splits when available.\n",
    "   - **File Path**: `data/SCOPe/version_${scope_version}/${dataset_name}/processed/${reader_name}/data.pt`\n",
    "   - **Additional File**: `splits.csv` - Contains saved splits for reproducibility.\n",
    "\n",
    "This structured approach to data management ensures that each stage of data processing is well-organized and documented, from raw data acquisition to the preparation of model-ready inputs. It also facilitates reproducibility and traceability across different experiments.\n",
    "\n",
    "### Data Splits\n",
    "\n",
    "- **Creation**: Data splits are generated dynamically \"on the fly\" during training and evaluation to ensure flexibility and adaptability to different tasks.\n",
    "- **Reproducibility**: To maintain consistency across different runs, splits can be reproduced by comparing hashes with a fixed seed value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e172c0d1e8bb93f",
   "metadata": {},
   "source": [
    "# 4. Data Files and their structure\n",
    "\n",
    "`chebai` creates and manages several data files during its operation. These files store various chemical data and metadata essential for different tasks. Letâ€™s explore these files and their content.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43329709-5134-4ce5-88e7-edd2176bf84d",
   "metadata": {},
   "source": [
    "## <u>raw files</u>\n",
    "- cla.txt, des.txt and hie.txt\n",
    "\n",
    "For detailed description of raw files and their structures, please refer the official website [here](https://scop.berkeley.edu/help/ver=2.08#parseablefiles-2.08).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558295e5a7ded456",
   "metadata": {},
   "source": [
    "## <u>data.pkl</u> File\n",
    "\n",
    "**Description**: Generated by the `prepare_data` method, this file contains processed data in a dataframe format. It includes the ids, sids which are used to label corresponding sequence, protein-chain sequence, and columns for each label with boolean values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd490270-59b8-4c1c-8b09-204defddf592",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-05T21:09:01.622317Z",
     "start_time": "2024-10-05T21:09:01.606698Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7d16247-092c-4e8d-96c2-ab23931cf766",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-05T21:11:51.296162Z",
     "start_time": "2024-10-05T21:11:44.559304Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the data (rows x columns):  (60424, 1035)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sids</th>\n",
       "      <th>sequence</th>\n",
       "      <th>class_46456</th>\n",
       "      <th>class_48724</th>\n",
       "      <th>class_51349</th>\n",
       "      <th>class_53931</th>\n",
       "      <th>class_56572</th>\n",
       "      <th>class_56835</th>\n",
       "      <th>class_56992</th>\n",
       "      <th>...</th>\n",
       "      <th>species_187294</th>\n",
       "      <th>species_56257</th>\n",
       "      <th>species_186882</th>\n",
       "      <th>species_56690</th>\n",
       "      <th>species_161316</th>\n",
       "      <th>species_57962</th>\n",
       "      <th>species_58067</th>\n",
       "      <th>species_267696</th>\n",
       "      <th>species_311502</th>\n",
       "      <th>species_311501</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[d4oq9a_, d4oq9b_, d4oq9c_, d4oq9d_, d4niaa_, ...</td>\n",
       "      <td>AAAAAAAAAA</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>[d7dxhc_]</td>\n",
       "      <td>AAAAAAAAAAAAAAAAAAAAAAA</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>[d1gkub1, d1gkub2, d1gkub3, d1gkub4]</td>\n",
       "      <td>AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAASLCLFPEDFLLKEF...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>[d3c9wa2, d3c9wb2, d3c9wa3, d3c9wb3]</td>\n",
       "      <td>AAAAAAGPEMVRGQVFDVGPRYTNLSYIGEGAYGMVCSAYDNLNKV...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>[d1xwaa1, d1xwab_, d1xwac_, d1xwad_, d1xwaa2]</td>\n",
       "      <td>AAAAAMVYQVKDKADLDGQLTKASGKLVVLDFFATWCGPCKMISPK...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 1035 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                               sids  \\\n",
       "0   1  [d4oq9a_, d4oq9b_, d4oq9c_, d4oq9d_, d4niaa_, ...   \n",
       "1   2                                          [d7dxhc_]   \n",
       "2   3               [d1gkub1, d1gkub2, d1gkub3, d1gkub4]   \n",
       "3   4               [d3c9wa2, d3c9wb2, d3c9wa3, d3c9wb3]   \n",
       "4   5      [d1xwaa1, d1xwab_, d1xwac_, d1xwad_, d1xwaa2]   \n",
       "\n",
       "                                            sequence  class_46456  \\\n",
       "0                                         AAAAAAAAAA        False   \n",
       "1                            AAAAAAAAAAAAAAAAAAAAAAA        False   \n",
       "2  AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAASLCLFPEDFLLKEF...        False   \n",
       "3  AAAAAAGPEMVRGQVFDVGPRYTNLSYIGEGAYGMVCSAYDNLNKV...        False   \n",
       "4  AAAAAMVYQVKDKADLDGQLTKASGKLVVLDFFATWCGPCKMISPK...        False   \n",
       "\n",
       "   class_48724  class_51349  class_53931  class_56572  class_56835  \\\n",
       "0         True        False        False        False        False   \n",
       "1        False        False        False        False         True   \n",
       "2        False         True        False         True        False   \n",
       "3        False        False         True        False        False   \n",
       "4        False         True        False        False        False   \n",
       "\n",
       "   class_56992  ...  species_187294  species_56257  species_186882  \\\n",
       "0        False  ...           False          False           False   \n",
       "1        False  ...           False          False           False   \n",
       "2        False  ...           False          False           False   \n",
       "3        False  ...           False          False           False   \n",
       "4        False  ...           False          False           False   \n",
       "\n",
       "   species_56690  species_161316  species_57962  species_58067  \\\n",
       "0          False           False          False          False   \n",
       "1          False           False          False          False   \n",
       "2          False           False          False          False   \n",
       "3          False           False          False          False   \n",
       "4          False           False          False          False   \n",
       "\n",
       "   species_267696  species_311502  species_311501  \n",
       "0           False           False           False  \n",
       "1           False           False           False  \n",
       "2           False           False            True  \n",
       "3           False           False            True  \n",
       "4           False           False            True  \n",
       "\n",
       "[5 rows x 1035 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pkl_df = pd.DataFrame(\n",
    "    pd.read_pickle(\n",
    "        os.path.join(\n",
    "            scope_class.processed_dir_main,\n",
    "            scope_class.processed_main_file_names_dict[\"data\"],\n",
    "        )\n",
    "    )\n",
    ")\n",
    "print(\"Size of the data (rows x columns): \", pkl_df.shape)\n",
    "pkl_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322bc926-69ff-4b93-9e95-5e8b85869c38",
   "metadata": {},
   "source": [
    "**File Path**: `data/SCOPe/version_${scope_version}/${dataset_name}/processed/data.pkl`\n",
    "\n",
    "\n",
    "### Structure of `data.pkl`\n",
    "`data.pkl` as following structure: \n",
    "- **Column 0**: Contains the ID of eachdata instance.\n",
    "- **Column 1**: Contains the `sids` which are associated with corresponding protein-chain sequence.\n",
    "- **Column 2**: Contains the protein-chain sequence.\n",
    "- **Column 3 and onwards**: Contains the labels, starting from column 3.\n",
    "\n",
    "This structure ensures that the data is organized and ready for further processing, such as further encoding.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba019d2d4324bd0b",
   "metadata": {},
   "source": [
    "## <u>data.pt</u> File\n",
    "\n",
    "\n",
    "**Description**: Generated by the `setup` method, this file contains encoded data in a format compatible with the PyTorch library, specifically as a list of dictionaries. Each dictionary in this list includes keys such as `ident`, `features`, `labels`, and `group`, ready for model input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "977ddd83-b469-4b58-ab1a-8574fb8769b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-05T21:12:49.338943Z",
     "start_time": "2024-10-05T21:12:49.323319Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3266ade9-efdc-49fe-ae07-ed52b2eb52d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-05T21:14:12.892845Z",
     "start_time": "2024-10-05T21:13:59.859953Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of loaded data: <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "data_pt = torch.load(\n",
    "    os.path.join(\n",
    "        scope_class.processed_dir, scope_class.processed_file_names_dict[\"data\"]\n",
    "    ),\n",
    "    weights_only=False,\n",
    ")\n",
    "print(\"Type of loaded data:\", type(data_pt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "84cfa3e6-f60d-47c0-9f82-db3d5673d1e7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-05T21:14:21.185027Z",
     "start_time": "2024-10-05T21:14:21.169358Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'features': [14, 14, 14, 14, 20, 15, 15, 28, 15, 18, 25, 17, 18, 11, 25, 21, 27, 19, 14, 27, 19, 13, 14, 17, 16, 21, 25, 22, 27, 28, 12, 10, 20, 19, 13, 13, 14, 28, 17, 20, 20, 12, 19, 11, 17, 15, 27, 28, 15, 12, 17, 14, 23, 11, 19, 27, 14, 26, 19, 11, 11, 19, 12, 19, 19, 28, 17, 16, 20, 16, 19, 21, 10, 16, 18, 12, 17, 19, 10, 29, 12, 12, 21, 20, 16, 17, 19, 28, 20, 21, 12, 16, 18, 21, 19, 14, 19, 17, 12, 14, 18, 28, 23, 15, 28, 19, 19, 19, 15, 25, 17, 22, 25, 19, 28, 16, 13, 27, 13, 11, 20, 15, 28, 12, 15, 28, 27, 13, 13, 13, 28, 19, 14, 15, 28, 12, 18, 14, 20, 28, 14, 18, 15, 19, 13, 22, 28, 29, 12, 12, 20, 29, 28, 17, 13, 28, 23, 22, 15, 15, 28, 17, 13, 21, 17, 27, 11, 20, 23, 10, 10, 11, 20, 15, 22, 21, 10, 13, 21, 25, 11, 29, 25, 19, 20, 18, 17, 19, 19, 15, 18, 16, 16, 25, 15, 22, 25, 28, 23, 16, 20, 21, 13, 26, 18, 21, 15, 27, 17, 20, 22, 23, 11, 14, 29, 21, 21, 17, 25, 10, 14, 20, 25, 11, 22, 29, 11, 21, 11, 12, 17, 27, 16, 29, 17, 14, 12, 11, 20, 21, 27, 22, 15, 10, 21, 20, 17, 28, 21, 25, 11, 18, 27, 11, 13, 11, 28, 12, 17, 23, 15, 25, 16, 20, 11, 17, 11, 12, 16, 28, 27, 27, 27, 14, 13, 16, 22, 28, 12, 12, 26, 19, 22, 21, 21, 12, 19, 28, 22, 16, 23, 20, 28, 27, 24, 15, 19, 13, 12, 12, 29, 28, 12, 20, 22, 23, 17, 17, 27, 27, 21, 20, 28, 28, 28, 14, 13, 13, 11, 14, 14, 14, 14, 14], 'labels': array([False,  True, False, ..., False, False, False]), 'ident': 6, 'group': None}\n"
     ]
    }
   ],
   "source": [
    "for i in range(5, 6):\n",
    "    print(data_pt[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d80ffbb-5f1e-4489-9bc8-d688c9be1d07",
   "metadata": {},
   "source": [
    "**File Path**: `data/SCOPe/version_${scope_version}/${dataset_name}/processed/${reader_name}/data.pt`\n",
    "\n",
    "\n",
    "### Structure of `data.pt`\n",
    "\n",
    "The `data.pt` file is a list where each element is a dictionary with the following keys:\n",
    "\n",
    "- **`features`**: \n",
    "  - **Description**: This key holds the input features for the model. The features are typically stored as tensors and represent the attributes used by the model for training and evaluation.\n",
    "\n",
    "- **`labels`**: \n",
    "  - **Description**: This key contains the labels or target values associated with each instance. Labels are also stored as tensors and are used by the model to learn and make predictions.\n",
    "\n",
    "- **`ident`**: \n",
    "  - **Description**: This key holds identifiers for each data instance. These identifiers help track and reference the individual samples in the dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186ec6f0eed6ecf7",
   "metadata": {},
   "source": [
    "## <u>classes.txt</u> File\n",
    "\n",
    "**Description**: A file containing the list of selected SCOPe **labels** based on the specified threshold. This file is crucial for ensuring that only relevant **labels** are included in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d1fbe6c-beb8-4038-93d4-c56bc7628716",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-05T21:15:19.146285Z",
     "start_time": "2024-10-05T21:15:18.503284Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class_48724\n",
      "class_53931\n",
      "class_310555\n",
      "fold_48725\n",
      "fold_56111\n",
      "fold_56234\n",
      "fold_310573\n",
      "superfamily_48726\n",
      "superfamily_56112\n",
      "superfamily_56235\n",
      "superfamily_310607\n",
      "family_48942\n",
      "family_56251\n",
      "family_191359\n",
      "family_191470\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(scope_class.processed_dir_main, \"classes.txt\"), \"r\") as file:\n",
    "    for i in range(15):\n",
    "        line = file.readline()\n",
    "        print(line.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861da1c3-0401-49f0-a22f-109814ed95d5",
   "metadata": {},
   "source": [
    "\n",
    "**File Path**: `data/SCOPe/version_${scope_version}/${dataset_name}/processed/classes.txt`\n",
    "\n",
    "The `classes.txt` file lists selected SCOPe classes. These classes are chosen based on a specified threshold, which is typically used for filtering or categorizing the dataset. Each line in the file corresponds to a unique SCOPe class ID, identifying specific class withing SCOPe ontology along with the hierarchy level.\n",
    "\n",
    "This file is essential for organizing the data and ensuring that only relevant classes, as defined by the threshold, are included in subsequent processing and analysis tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb72be449e52b63f",
   "metadata": {},
   "source": [
    "## <u>splits.csv</u> File\n",
    "\n",
    "**Description**: Contains saved data splits from previous runs. During subsequent runs, this file is used to reconstruct the train, validation, and test splits by filtering the encoded data (`data.pt`) based on the IDs stored in `splits.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ebdcae4-4344-46bd-8fc0-a82ef5d40da5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-05T21:15:54.575116Z",
     "start_time": "2024-10-05T21:15:53.945139Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  split\n",
       "0   1  train\n",
       "1   3  train\n",
       "2   4  train\n",
       "3   6  train\n",
       "4   9  train"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_df = pd.read_csv(os.path.join(scope_class.processed_dir_main, \"splits.csv\"))\n",
    "csv_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b058714f-e434-4367-89b9-74c129ac727f",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**File Path**: `data/SCOPe/version_${scope_version}/${dataset_name}/processed/splits.csv`\n",
    "\n",
    "The `splits.csv` file contains the saved data splits from previous runs, including the train, validation, and test sets. During subsequent runs, this file is used to reconstruct these splits by filtering the encoded data (`data.pt`) based on the IDs stored in `splits.csv`. This ensures consistency and reproducibility in data splitting, allowing for reliable evaluation and comparison of model performance across different run.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6dc3fd6c-7cf6-47ef-812f-54319a0cdeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can specify a literal path for the `splits_file_path`, or if another `scope_class` instance is already defined,\n",
    "# you can use its existing `splits_file_path` attribute for consistency.\n",
    "scope_class_with_splits = SCOPeOver2000(\n",
    "    scope_version=\"2.08\",\n",
    "    # splits_file_path=\"data/chebi_v231/ChEBI50/processed/splits.csv\",  # Literal path option\n",
    "    splits_file_path=scope_class.splits_file_path,  # Use path from an existing `chebi_class` instance\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5eb482c-ce5b-4efc-b2ec-85ac7b1a78ee",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab110764-216d-4d52-a9d1-4412c8ac8c9d",
   "metadata": {},
   "source": [
    "## 5.1 Protein Representation Using Amino Acid Sequence Notation\n",
    "\n",
    "Proteins are composed of chains of amino acids, and these sequences can be represented using a one-letter notation for each amino acid. This notation provides a concise way to describe the primary structure of a protein.\n",
    "\n",
    "### Example Protein Sequence\n",
    "\n",
    "Protein-Chain: PDB ID:**1cph** Chain ID:**B** mol:protein length:30  INSULIN (PH 10)\n",
    "</br>Refer - [1cph_B](https://www.rcsb.org/sequence/1CPH)\n",
    "\n",
    "- **Sequence**: `FVNQHLCGSHLVEALYLVCGERGFFYTPKA`\n",
    "- **Sequence Length**: 30\n",
    "\n",
    "In this sequence, each letter corresponds to a specific amino acid. This notation is widely used in bioinformatics and molecular biology to represent protein sequences.\n",
    "\n",
    "### Tokenization and Encoding\n",
    "\n",
    "To tokenize and numerically encode this protein sequence, the `ProteinDataReader` class is used. This class allows for n-gram tokenization, where the `n_gram` parameter defines the size of the tokenized units. If `n_gram` is not provided (default is `None`), each amino acid letter is treated as a single token.\n",
    "\n",
    "For more details, you can explore the implementation of the `ProteinDataReader` class in the source code [here](https://github.com/ChEB-AI/python-chebai/blob/dev/chebai/preprocessing/reader.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da47d47e-4560-46af-b246-235596f27d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from chebai.preprocessing.reader import ProteinDataReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8bdbf309-29ec-4aab-a6dc-9e09bc6961a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "protein_dr_3gram = ProteinDataReader(n_gram=3)\n",
    "protein_dr = ProteinDataReader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "68e5c87c-79c3-4d5f-91e6-635399a84d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[25, 28, 19, 18, 29, 17, 24, 13, 11, 29, 17, 28, 27, 14, 17, 22, 17, 28, 24, 13, 27, 16, 13, 25, 25, 22, 15, 23, 21, 14]\n",
      "[5023, 2218, 3799, 2290, 6139, 2208, 6917, 4674, 484, 439, 2737, 851, 365, 2624, 3240, 4655, 1904, 3737, 1453, 2659, 5160, 3027, 2355, 7163, 4328, 3115, 6207, 1234]\n"
     ]
    }
   ],
   "source": [
    "protein = \"FVNQHLCGSHLVEALYLVCGERGFFYTPKA\"\n",
    "print(protein_dr._read_data(protein))\n",
    "print(protein_dr_3gram._read_data(protein))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7211ee-2ccc-46d3-8e8f-790f344726ba",
   "metadata": {},
   "source": [
    "The numbers mentioned above refer to the index of each individual token from the [`tokens.txt`](https://github.com/ChEB-AI/python-chebai/blob/dev/chebai/preprocessing/bin/protein_token/tokens.txt) file, which is used by the `ProteinDataReader` class. \n",
    "\n",
    "Each token in the `tokens.txt` file corresponds to a specific amino-acid letter, and these tokens are referenced by their index. Additionally, the index values are offset by the `EMBEDDING_OFFSET`, ensuring that the token embeddings are adjusted appropriately during processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e328cf-09f9-4694-b175-28320590937d",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
