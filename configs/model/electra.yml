class_path: chebai.models.Electra
init_args:
  optimizer_kwargs:
    lr: 1e-3
  config:
    vocab_size: 31  # 21 amino acids (when n_gram=1) + 10 special tokens of LLM
    max_position_embeddings: 1000  # max default sequence length for protein
    num_attention_heads: 8
    num_hidden_layers: 6
    type_vocab_size: 1
    hidden_size: 256
